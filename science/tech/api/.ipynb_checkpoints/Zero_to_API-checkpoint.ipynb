{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero to API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)Table Of Contents  \n",
    "==\n",
    "\n",
    "[Introduction](#Introduction)  \n",
    "[DynamoDB](#DynamoDB)  \n",
    "[Lambda](#Lambda)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a guide to build a serverless API using AWS lambda. The entire pipeline from data curation to data retrival is built using AWS products like S3 bucket, Dynamodb and lambda.\n",
    "![Zero to API](https://i.imgur.com/qiCcCNL.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon DynamoDB is a nonrelational database. Data is read in the form of dictionary.\n",
    "DynamoDB involves table creation with two attributes:\n",
    "\n",
    "    1) Partition Key - For our case it is 'indiv'\n",
    "    2) Sort Key - For our case it is 'time' (Optional)\n",
    "Sort key is optional but can improve query time if the column is likely to be queried in the API.\n",
    "\n",
    "DynamoDB only allows query parameters with Partition key and Sort key, which means you cannot query for a\n",
    "column which is neither Partition or Sort key.\n",
    "\n",
    "Ex- Our data has columns ['time','x', 'y', 'indiv'] filtering and subsetting can only be done on our Partition key('indiv')\n",
    "and sort key('time') and not on either 'x' or 'y', so a query to fetch all 'x' == 728.2 won't work.\n",
    "\n",
    "DynamoDB sets read and write limit which are both defaulted to 5 hits. This can be increase or decreased based on API requiremnt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Data from S3 to Dynamodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done creating a [DynamoDB](https://aws.amazon.com/dynamodb/) table using the GUI on AWS, next step involves reading the csv from S3 bucket to this newly created table.\n",
    "\n",
    "For the data upload since the default write limit(5) would be low for 1 million rows to be transferred we'll increase this number to 1000 on the DynamoDB GUI ![DaynamoDB](https://i.imgur.com/EzC3t8R.png)\n",
    "\n",
    "We have created a table called 'baboons' with Partition Key = 'indiv' and Sort Key = 'time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done trasnferring write capacity units should be set back to lower values for lesser cost(default =5).\n",
    "\n",
    "Since we have a million rows to process we will subset out data files into small chunks(10 for this example) and batch process them to make upload faster.\n",
    "\n",
    "The run time for this would be around 15mins, without multiprocesing it would around 150mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3  # Library to transfer files to DynamoDB\n",
    "from tqdm import tqdm #Library to get a progress bar\n",
    "import numpy as np\n",
    "from multiprocessing import Pool # Multiprocess data upload.\n",
    "\n",
    "def dynamodb_upload(data_frame):\n",
    "    col_to_string = ['x', 'y', 'indiv'] #converting columns to string for DynamoDB\n",
    "    for i in col_to_string:                                                                                                                        \n",
    "        data_frame[i] = data_frame[i].astype(str)\n",
    "    mydata=data_frame.T.to_dict().values() # converting dataframe to a dict for DynamoDB to consume\n",
    "    MY_ACCESS_KEY_ID = ''\n",
    "    MY_SECRET_ACCESS_KEY = ''\n",
    "    resource = boto3.resource('dynamodb', aws_access_key_id=MY_ACCESS_KEY_ID, aws_secret_access_key=MY_SECRET_ACCESS_KEY, region_name='us-east-1')\n",
    "    table = resource.Table('baboons') # your table name where you want to push the data\n",
    "    with table.batch_writer() as batch: # batch writer\n",
    "        for row in tqdm(mydata):    #progress bar                                                                                                               \n",
    "            batch.put_item(Item=row) \n",
    "\n",
    "if __name__=='__main__':\n",
    "    data = pd.read_csv('https://s3-us-west-2.amazonaws.com/himatdata/baboons/baboons_ritxy.csv') #S3 location of baboons dataset\n",
    "    split_df = np.array_split(data, 10) # splitting data into 10 smaller data frames.\n",
    "    pool = Pool(processes=10)\n",
    "    pool.map(dynamodb_upload, split_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Lambda lets you run code without provisioning or managing servers and help building a serverless API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda provides lightweight serverless way to serve an API. One downside is it doens't come with all python libraries except for the base packages and boto(Aws package). In order to use any other package we have to zip the package alongside our 'lambda_function.py' file for it to work.\n",
    "\n",
    "In our case we need 'json2html' so we need to zip json2html folder alongside the our 'lambda_function.py' for it to work and uplaod it to lamdba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build the zip file for our lamdba function.\n",
    "\n",
    "1)create a new virtual env with conda - conda create -n my_new_env_name.\n",
    "\n",
    "2)activate the new env created - soure activate my_new_env_name.\n",
    "\n",
    "3)pip install the package you might need for your lambda to work- pip install json2html.\n",
    "\n",
    "4)grab the json2html folder. Mostly it would be in the site-packages path of anaconda ex- '/home/shivraj/anaconda3/lib/python3.6/site-packages/json2html'.\n",
    "\n",
    "5)zip the folder json2html obtained from step4 and also your lambda_function.py which we'll talk about.\n",
    "\n",
    "6)Upload the zip file in the lambda AWS UI.\n",
    "\n",
    "This is how the UI of lambda would look like:\n",
    "![lambda_ui](https://i.imgur.com/9KFK665.png)\n",
    "\n",
    "This now has the required package json2html as folder which lambda can read from and our main module lambda_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lambda_function.py \n",
    "This lambda function builds API interface to interact with our DynamoDB and return requested data based on query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "import boto3\n",
    "from pprint import pformat\n",
    "from json2html import *\n",
    "\n",
    "MY_SECRET_ACCESS_KEY = '' \n",
    "MY_ACCESS_KEY_ID = ''\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    dynamodb = boto3.resource('dynamodb', aws_access_key_id=MY_ACCESS_KEY_ID, aws_secret_access_key=MY_SECRET_ACCESS_KEY, region_name='us-east-1')\n",
    "    table = dynamodb.Table('baboons')\n",
    "    baboon = str(event[\"queryStringParameters\"]['indiv']) # parse api call to get indiv\n",
    "    d0 = str(event[\"queryStringParameters\"]['d0']) # parse api call to get start time\n",
    "    dt = str(event[\"queryStringParameters\"]['dt']) # parse api call to get end time\n",
    "    data_frame_flag = event[\"queryStringParameters\"]['table'].lower() == \"true\" # parse api call to get table flag\n",
    "    # query dynamoDB for the parsed filter parameters and return a response.\n",
    "    response = table.query(KeyConditionExpression=Key('indiv').eq(baboon) & Key('time').between(d0, dt))\n",
    "\n",
    "    dict_string = pformat(response['Items']) #prettify the dict for asthetics\n",
    "    #conditional return a html table based on table flag in the api call.\n",
    "    if not data_frame_flag:\n",
    "        print(\"Returning JSON\")\n",
    "        return { \"statusCode\": 200, \"body\": dict_string }\n",
    "    else:\n",
    "        print(\"Returning HTML\")\n",
    "        return { \n",
    "            \"statusCode\": 200, \n",
    "            \"body\": json2html.convert(response['Items']),  \n",
    "            \"headers\": {\n",
    "        'Content-Type': 'text/html',\n",
    "    }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample API call would be: https://ezmt8rznkh.execute-api.us-east-1.amazonaws.com/default/baboon?indiv=1&table=true&d0=0:02:52&dt=0:02:58\n",
    "\n",
    "Here params are:\n",
    "\n",
    "indiv=1 (to get baboons with id 1)\n",
    "\n",
    "table=true/false (whether to return a json or html formatted data)\n",
    "\n",
    "d0= start time ( time from which data required)\n",
    "\n",
    "dt= end time (time till which data should be queried)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
