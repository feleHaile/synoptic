{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# dual origins, no dough, data system operation, acceleration \n",
    "\n",
    "import sys\n",
    "print('\\nHello, Jupyter Notebook fans! I am running Python {}...'.format(sys.version_info[0]))\n",
    "print('\\n...on a M5-Large EC2 instance on the Amazon Web Services public cloud.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**abstract**\n",
    "\n",
    "> There is growing demand by UW faculty for research-aligned computing environments for coursework: For statistics, big data methods, data management, high performance (cluster) computing. This session will describe our early progress on the public cloud with emphasis on the strengths of JupyterHub, kubernetes and Python and explore the rapidly expanding ecosystem of cloud services and technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# By Jupyter\n",
    "\n",
    "\n",
    "## Narrative\n",
    "\n",
    "Including [links to things like the GLODAP snapshot](https://en.wikipedia.org/wiki/Global_Ocean_Data_Analysis_Project)...\n",
    "\n",
    "\n",
    "...and equations like $\\Large{\\;\\;\\;\\;e^{i\\theta}+1=0}$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...and citations...\n",
    "\n",
    "> Hosseinzadeh, P., Bhardwaj, G., Mulligan, V.K., Shortridge, M.D., Craven, T.W., Pardo-Avila, F., Rettie, S.A., Kim, D.E., Silva, D.A., Ibrahim, Y.M. and Webb, I.K., 2017. Comprehensive computational design of ordered peptide macrocycles. **Science**, 358(6369), pp.1461-1466.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# images\n",
    "!cp ./../kilroy/kilroy.py .\n",
    "import kilroy\n",
    "kilroy.Show('landscape','David_Shean_using_UW_terrestrial_laser_scanner_at_South_Cascade_Glacier.png', 600,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...and images.  Here is *David Shean just prior to being rescued by eagles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Video content from YouTube\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('MLM0fjy8Vz8')\n",
    "# https://youtu.be/G0DCb3lH_uU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the sounds of the ocean including diel vertical migration or DVM\n",
    "YouTubeVideo('HdHW77blulg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"./../videos/msla_the_movie.mp4\" type=\"video/mp4\">\n",
    "</video></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"../audio/Hydrophone_example.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sources\n",
    "\n",
    "* [Heidi Sosik's Imaging Flow Cytometer (WHOI)](http://ifcb-data.whoi.edu/mvco)\n",
    "  * [Same but API-specific](http://ifcb-data.whoi.edu/api)\n",
    "* [BCO-DMO (WHOI)](https://www.bco-dmo.org/data)\n",
    "* ARGO\n",
    "  * [GitHub pyARGO](https://github.com/castelao/pyARGO) Rob is not convinced this is complete enough to be useful...\n",
    "  * [UCSD ARGO Informational FAQ](http://www.argo.ucsd.edu/Data_FAQ.html#RorD)\n",
    "  * [Coriolis map-based data selection tool](http://www.argodatamgt.org/Access-to-data/Argo-data-selection)\n",
    "  * [xarray lesson that works with ARGO data](https://rabernat.github.io/research_computing/xarray.html)\n",
    "* GLODAP\n",
    "* AQUA\n",
    "* TERRA\n",
    "* OOI CA CI\n",
    "* ECCO2\n",
    "* LIVE OCEAN\n",
    "* NANOOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So to conclude\n",
    "\n",
    "- Jupyter notebooks can be managed as open source narratives \n",
    "  - Python\n",
    "  - Community\n",
    "\n",
    "\n",
    "Oceanography\n",
    "\n",
    "\n",
    "microphones, sonar, chemistry, temperature, oxygen, pH, seismics, cameras...\n",
    "\n",
    "\n",
    "\n",
    "#### This notebook will persist on GitHub\n",
    "\n",
    "- utility code\n",
    "- images, videos, audio, links, citations\n",
    "- invoke software packages\n",
    "- cloud (relax you're in it already)\n",
    "- local file system\n",
    "- Deconstruct \n",
    "- Widget display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech versus Solution\n",
    "\n",
    "\n",
    "- Azure and Google both push **hosted environments**\n",
    "- Making those work can beat learning...\n",
    "  - Anaconda\n",
    "  - Kubernetes\n",
    "  - HELM\n",
    "  - KOPS\n",
    "  - Docker / DockerHub\n",
    "  - git / GitHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this notebook: pandas, netcdf4, xarray and boto must be installed (terminal or via !)\n",
    "# !conda install netcdf4 -y\n",
    "# !conda install xarray -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import boto\n",
    "from boto.s3.key import Key\n",
    "import xarray as xr\n",
    "\n",
    "# This cell copies a glodap salinity file to the local directory, naming it 'glodap_salinity.nc'\n",
    "connection = boto.connect_s3(anon=True)\n",
    "bucket = connection.get_bucket('himatdata')\n",
    "data_dir = './../data/'\n",
    "\n",
    "# This block of bespoke-y code prints all the root directory names from the AWS S3 'himatdata' bucket\n",
    "dir_list = []\n",
    "for key in bucket.list():\n",
    "    keyname = str(key.name.encode('utf-8'))\n",
    "    if '/' in keyname:\n",
    "        cd = keyname.split('/')[0].strip(\"b'\").strip('b\"')\n",
    "        if cd not in dir_list:\n",
    "            dir_list.append(cd)\n",
    "            print(cd)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Now let's list all the contents of that glodap directory\n",
    "file_list = []\n",
    "for key in bucket.list():\n",
    "    keyname = str(key.name.encode('utf-8'))\n",
    "    if 'glodap/' in keyname:\n",
    "        cd = keyname.strip(\"b'\").strip('b\"')\n",
    "        print(cd)\n",
    "\n",
    "print('\\n')\n",
    "for key in bucket.list(): \n",
    "    filename = key.name.encode('utf-8')\n",
    "    if b'glodap' in filename: \n",
    "        if b'salinity.nc' in filename: \n",
    "            print ('salinity file is', filename)\n",
    "            salinityfilename = filename\n",
    "        if b'temperature.nc' in filename: \n",
    "            print ('temperature file is', filename)\n",
    "            temperaturefilename = filename\n",
    "        if b'oxygen.nc' in filename: \n",
    "            print('oxygen file is', filename)\n",
    "            oxygenfilename = filename            \n",
    "\n",
    "local_salinity_filename = data_dir + 'glodap_salinity.nc'\n",
    "local_temperature_filename = data_dir + 'glodap_temperature.nc'\n",
    "local_oxygen_filename = data_dir + 'glodap_oxygen.nc'\n",
    "\n",
    "print('\\n')\n",
    "!ls -al ./../data/*.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python code copies three data files from the cloud to the local file system\n",
    "should_i_get_these_files = False\n",
    "if should_i_get_these_files:\n",
    "    k = Key(bucket)\n",
    "    k.key = salinityfilename\n",
    "    k.get_contents_to_filename(local_salinity_filename)\n",
    "    k.key = temperaturefilename\n",
    "    k.get_contents_to_filename(local_temperature_filename)\n",
    "    k.key = oxygenfilename\n",
    "    k.get_contents_to_filename(local_oxygen_filename)\n",
    "\n",
    "# If this works you should see three 100MByte files listed\n",
    "!ls -al ./../data/glodap_*.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assigns our data files to three distinct dataset objects\n",
    "print (local_salinity_filename)\n",
    "dsSal = xr.open_mfdataset(local_salinity_filename)\n",
    "dsTemp = xr.open_mfdataset(local_temperature_filename)\n",
    "dsO2 = xr.open_mfdataset(local_oxygen_filename)\n",
    "dsO2          # This prints the structure of the oxygen dataset below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tears of frustration (breadcrumbs)\n",
    "\n",
    "\n",
    "Above some **Attributes** are truncated. I skip over the 'thrashing' (like dsO2['Attributes']['Description']) \n",
    "and cut to the chase: xarray Dataset **Attributes** print nicely as follows:\n",
    "\n",
    "```\n",
    "print(dsO2.Description)\n",
    "print(dsO2.Citation)\n",
    "```\n",
    "\n",
    "But what are the units of dissolved oxygen? Upon reflection it would make sense to associate units \n",
    "directly with the data. I use the 'dir' Python directory function to break down the Dataset into components. \n",
    "Indeed dsO2 (as a Dataset) has a component called oxygen which in turn has a sub-component called units..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dsO2.Comment) # garbled text. Norwegian?\n",
    "# print('\\n')\n",
    "print(dsO2.Description)             # The description is fine as far as it goes but does not indicate units.\n",
    "print('\\n')\n",
    "print(dsO2.Citation)\n",
    "\n",
    "# Run the 'directory' operation on dsO2 to find oxygen; on dsO2.oxygen to find units; then...\n",
    "#dir(dsO2.oxygen.units)\n",
    "\n",
    "# print units\n",
    "print(\"\\n\" + dsO2.oxygen.units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Python **inspect** module? Let's give that a whirl..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getdoc(dsO2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widget Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code indexes into the Oxygen dataset and prints a few example oxygen values\n",
    "dsO2['oxygen'][0:2,90:92,120:122].values   # a few dissolved oxygen values near the surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsTemp['temperature'][0,20:24,160:200].values.mean())\n",
    "print(dsTemp['temperature'][0,80:100,160:200].values.mean())\n",
    "# dir(dsTemp['temperature'][0,170:174,320:324].values.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports give us control sliders that we use for selecting depth slices from the dataset\n",
    "from ipywidgets import *\n",
    "from traitlets import dlink\n",
    "\n",
    "# This creates a 2D color-coded view of oxygen at the surface, attaching a slider to a depth parameter\n",
    "def plotOxygen(depth_index):\n",
    "    a=dsO2['oxygen'].sel(depth_surface = depth_index)\n",
    "    a.plot(figsize=(16, 10),cmap=plt.cm.bwr,vmin=150, vmax=350)\n",
    "    msg = 'This is for '\n",
    "    if depth_index == 0: msg += 'surface water'\n",
    "    else: msg += 'water at ' + str(int(dsO2['Depth'].values[depth_index])) + ' meters depth'\n",
    "    plt.text(25, -87, msg, fontsize = '20')\n",
    "    plt.text(28, 50, 'oxygen dissolved in', fontsize = '20')\n",
    "    plt.text(28, 42, '     ocean water   ', fontsize = '20')\n",
    "\n",
    "# This is the interactive slider\n",
    "interact(plotOxygen, depth_index=widgets.IntSlider(min=0,max=32,step=1,value=0, continuous_update=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSalinity(depth_index):\n",
    "    b = dsSal['salinity'].sel(depth_surface = depth_index)\n",
    "    b.plot(figsize=(16, 10),cmap=plt.cm.bwr,vmin=33, vmax=36)\n",
    "    msg = 'This is for '\n",
    "    if depth_index == 0: msg += 'surface water'\n",
    "    else: msg += 'water at ' + str(int(dsO2['Depth'].values[depth_index])) + ' meters depth'\n",
    "    plt.text(25, -87, msg, fontsize = '20')\n",
    "    plt.text(47, 50, 'salinity of', fontsize = '20')\n",
    "    plt.text(47, 42, 'ocean water', fontsize = '20')\n",
    "    \n",
    "interact(plotSalinity, depth_index=widgets.IntSlider(min=0,max=32,step=1,value=0, continuous_update=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTemperature(depth_index):\n",
    "    c=dsTemp['temperature'].sel(depth_surface = depth_index)\n",
    "    c.plot(figsize=(16, 10),cmap=plt.cm.bwr,vmin=0, vmax=23)\n",
    "    msg = 'This is for '\n",
    "    if depth_index == 0: msg += 'surface water'\n",
    "    else: msg += 'water at ' + str(int(dsO2['Depth'].values[depth_index])) + ' meters depth'\n",
    "    plt.text(25, -87, msg, fontsize = '20')\n",
    "    plt.text(47, 50, 'temperature of', fontsize = '20')\n",
    "    plt.text(47, 42, 'ocean water', fontsize = '20')\n",
    "\n",
    "interact(plotTemperature, depth_index=widgets.IntSlider(min=0,max=32,step=1,value=0, continuous_update=False))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code pulls out the various available depths (in meters) of the dataset indexed slices\n",
    "print (dsTemp['Depth'].values[10])\n",
    "print ('    ')\n",
    "print (dsTemp['Depth'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transit to Curriculum\n",
    "\n",
    "- Moby runs on AWS T2-Large (cheap!)\n",
    "- This Jupyter Hub is useful to **research** ~ marina. \n",
    "- Now Faculty must teach classes of $N = (2 - 5000)$ \n",
    "- Requirements:\n",
    "  - Each of N > URL > NetID\n",
    "  - They “arrive” at a website pre-populated \n",
    "  - They start and can start modifying notebooks, upload files, etcetera\n",
    "  - Scikit-learn already installed, NetworkX, boto, NetCDF4, ...\n",
    "  - My students will need access to \n",
    "    - NVIDIA GPUs for their deep neural net lesson\n",
    "    - (small) cluster for their HPC lesson\n",
    "    - $\n",
    "    - NOT: other students’ work\n",
    "  - No student credit cards\n",
    "  - I can grade my students’ work \n",
    "  - 5 TB of data... \n",
    "  - When my student accidentally deletes everything...\n",
    "  - My students never have to wait more than 2 minutes to access their Server\n",
    "  - I need to update half-way through the course\n",
    "  - How To Pay For This???\n",
    "  - My total cost to run this course should be as low as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine\n",
    "\n",
    "The public cloud { one of AWS, Google Cloud, Azure }\n",
    "Scaling technologies { docker, DockerHub, kubernetes, KOPS, HELM, please advise }\n",
    "Replication technologies { JupyterHub, GitHub, nbgitpuller, please advise } \n",
    "Storage {Each user has a persistent disk space or use EFS (unstable) or use buckets on Google }\n",
    "Programming language including package manager { Python 3, Anaconda }\n",
    "Authentication, OAuth through Github, Google etc. \n",
    "\n",
    "The faculty/instructor gets access to a cloud-hosted Jupyter notebook and begins installing tools and creating notebooks\n",
    "Much of this is the domain of the eScience Institute\n",
    "A consultant (ATL) configures the JHub\n",
    "A miracle happens (please elaborate)\n",
    "The students pop laptops at the first class and log in, learn the ropes\n",
    "Here is how the instructor grades the first assignment...\n",
    "After three weeks one student notices some graffiti on a wall and decides to see what it does: ‘rm -r *’\n",
    "Two days before the final project is due 79 students simultaneously jump into their notebooks and begin spinning up clusters… here is what happens…’\n",
    "\n",
    "\n",
    "AT’s notes: Merge as see fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Two traditional ways of a Jupyterhub deployment: \n",
    "\n",
    "(a) a single giant server where packages and environments are installed directly onto the machine via pip install or conda install. Pros: easy to set up, can quickly install packages as the need arises. Cons: works for small groups, on work that are not memory/compute intensive. \n",
    "\n",
    "(b) scalable using a container management and load balancing.  Container management = Docker + Kubernetes + Helm + KOPS... Pros: scalable, can accommodate a large number of participants. Cons: uses docker images to create environments, therefore needs a longer lead time to ensure all packages, libraries and dependencies are installed; not feasible to do things on the fly \n",
    "\n",
    "Installation/setup requirements: \n",
    "\n",
    "For (a) provision a large server - say one with 48 cpus and 192 GB memory. Follow the steps outlined in https://jupyterhub.readthedocs.io/en/latest/installation-guide.html to install jupyterhub. Thereafter, packages and libraries can be installed on the server using the traditional pip install or conda install. This will propagate as “kernels” on Jupyterhub. Each user is assigned a home directory on the server which corresponds to their login username (i.e. if Github is the authentication mechanism, then the home directories should correspond exactly to the Github username)\n",
    "\n",
    "For (b), the installation and setup are outlined in https://zero-to-jupyterhub-with-kubernetes.readthedocs.io/en/latest/\n",
    "\n",
    "The fundamental idea here is that instead of having a giant server sitting on standby, the number of servers can be scaled up or down as required. Four main technologies used in this version of deployment:\n",
    "\n",
    "Kubernetes which is a manager for the deployment and scaling of containers. Kubernetes clusters are the foundation of the scalable Jupyterhub ecosystem -- it consists of a master node where the Jupyterhub spawner resides and worker nodes where “pods” or docker containers are spawned. \n",
    "\n",
    "Kubernetes clusters are provisioned on Google Cloud Platform using its native API, Azure using Azure container services and AWS using two methods - KOPS (Kubernetes operations service) or a quickstart Heptio Template. KOPS autoscales while deployment of kubernetes clusters using the Heptio template does NOT autoscale. \n",
    "\n",
    "\n",
    "Once the kubernetes cluster is provisioned, the kubernetes command tool (kubectl) needs to be installed in order to interact the kubernetes cluster. The distinction between kubectl and kubernetes management tools like KOPs is that kubectl cannot provision a kubernetes cluster. It is mainly used to issue commands TO a pre-made kubernetes cluster.  Kubectl helps you get cluster information like node IPs and # of pods or to figure out if something went wrong with your kubernetes cluster. \n",
    "Jupyterhub is installed on the kubernetes cluster using helm. Helm is an application manager for kubernetes. \n",
    "Once a user logs on to your scalable Jupyterhub, a “pod” starts up. What a pod is, is essentially a docker container. Jupyterhub pulls a docker image from an image that you have created; this image contains all your packages and libraries that you want to use for your class. The path to this image is specified in a configuration file while installing Jupyterhub via helm. This docker image can also contain *some* data though not encouraged to be large datasets. \n",
    "\n",
    "so you do a a helm install jupyterhub\n",
    "to actually install jupyterhub on the kubernetes cluster\n",
    "\n",
    "Additional technology to understand: Docker\n",
    "\n",
    "Things to consider: You can specify how much CPU, memory and disk space to allocate to each user. This will factor into the size and number of machines you choose. For example, let’s say you have 150 students in your course and each student will be using 4 cpus and 6GiB of memory. This will mean that your maximum cluster size should be about 17 c4.8xlarge instances (on AWS). That will allocate space for about 9 “pods” per instance. \n",
    "\n",
    "Ways to democratize the process: (a) we create a deployjhub package that automatically provisions a kubernetes cluster, installs jupyterhub and deploys the docker image. (b) training a teaching assistant to spin up the instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dimensions names and coordinate indexes to numpy’s ndarray makes many powerful array operations possible:\n",
    "\n",
    "Apply operations over dimensions by name: x.sum('time').\n",
    "Select values by label instead of integer location: x.loc['2014-01-01'] or x.sel(time='2014-01-01').\n",
    "Mathematical operations (e.g., x - y) vectorize across multiple dimensions (array broadcasting) based on dimension names, not shape.\n",
    "Flexible split-apply-combine operations with groupby: x.groupby('time.dayofyear').mean().\n",
    "Database like alignment based on coordinate labels that smoothly handles missing values: x, y = xr.align(x, y, join='outer').\n",
    "Keep track of arbitrary metadata in the form of a Python dictionary: x.attrs.\n",
    "pandas provides many of these features, but it does not make use of dimension names, and its core data structures are fixed dimensional arrays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
