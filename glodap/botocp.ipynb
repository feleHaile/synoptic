{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "# boto to move files to and from AWS object storage (S3)\n",
    "\n",
    "### copying glodap data files from Amazon public cloud object storage to make local copies\n",
    "\n",
    "This notebook copies files from AWS S3 buckets to ```/home/jovyan/data```, i.e. the local Jupyter workspace.\n",
    "This *should not* be done when direct access to S3 is possible; but we are setting aside that aesthetic here.\n",
    "Copying from AWS object storage S3 buckets *should* in fact be done using **boto3**, which is distinct from\n",
    "its predecessor **boto**. However I don't have time to make that change at the moment; so we proceed with\n",
    "two lost style points using just plain **boto**.\n",
    "\n",
    "\n",
    "By the by Stack Overflow says:\n",
    "\n",
    "> The boto package is the hand-coded Python library that has been around \n",
    "since 2006. It is very popular and is fully supported by AWS but because \n",
    "it is hand-coded and there are so many services available (with more \n",
    "appearing all the time) it is difficult to maintain.\n",
    "> \n",
    "> So, boto3 is a new version of the boto library based on botocore. All \n",
    "of the low-level interfaces to AWS are driven from JSON service descriptions \n",
    "that are generated automatically from the canonical descriptions of the services. \n",
    "So, the interfaces are always correct and always up to date. There is a \n",
    "resource layer on top of the client-layer that provides a nicer, more Pythonic interface.\n",
    ">\n",
    ">The boto3 library is being actively developed by AWS and \\[is therefore recommended\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Python utility code\n",
    "from pathlib import Path\n",
    "home_d = str(Path.home()) + '/'\n",
    "data_d = home_d + 'data/'             # A non-repository location for datasets of interest\n",
    "\n",
    "def dirobj(obj): return [x for x in dir(obj) if not x.startswith('__')]\n",
    "\n",
    "def lsal(path=''):\n",
    "    import os\n",
    "    return os.popen('ls -al ' + path).readlines()\n",
    "\n",
    "def ShowGitHubImage(username, repo, folder, source, localpath, localname, width, height):\n",
    "    global home_d\n",
    "    import requests, shutil\n",
    "    from PIL import Image\n",
    "    outf = localpath + '/' + localname\n",
    "    f = 'https://raw.githubusercontent.com/' + username + '/' + repo + '/master/' + folder + '/' + source\n",
    "    a = requests.get(f, stream = True)\n",
    "    if a.status_code == 200:\n",
    "        with open(outf, 'wb') as f:\n",
    "            a.raw.decode_content = True\n",
    "            shutil.copyfileobj(a.raw, f)\n",
    "    return Image.open(outf).resize((width,height),Image.ANTIALIAS)\n",
    "\n",
    "def ShowLocalImage(path, filename, width, height):\n",
    "    from PIL import Image\n",
    "    f = path + '/' + filename \n",
    "    return Image.open(f).resize((width,height),Image.ANTIALIAS)\n",
    "\n",
    "# Test either of the 'Show Image' functions\n",
    "# ShowGitHubImage('robfatland', 'othermathclub', 'images/cellular', 'conus_textile_shell_2.png', home_d, 'ctextile.jpg', 450, 250)\n",
    "# ShowLocalImage(home_d, 'ctextile.jpg', 450, 250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Don't run unless you want to grab glodap files from S3\n",
    "# This may take a couple minutes to run\n",
    "# It fails if ~/data/glodap does not exist; so it should really check for that and create it\n",
    "# The other key point here is that we should be using boto3\n",
    "\n",
    "import boto\n",
    "\n",
    "if False:\n",
    "    data_dir = data_d + '/glodap/'\n",
    "    local_salinity_filename = data_dir + 'glodap_salinity.nc'\n",
    "    local_temperature_filename = data_dir + 'glodap_temperature.nc'\n",
    "    local_oxygen_filename = data_dir + 'glodap_oxygen.nc'\n",
    "\n",
    "    connection = boto.connect_s3(anon=True)\n",
    "    bucket = connection.get_bucket('himatdata')\n",
    "    for key in bucket.list():\n",
    "        keyname = str(key.name.encode('utf-8'))\n",
    "        if 'glodap/' in keyname and 'salinity' in keyname: key.get_contents_to_filename(local_salinity_filename)\n",
    "        elif 'glodap/' in keyname and 'temperature' in keyname: key.get_contents_to_filename(local_temperature_filename)\n",
    "        elif 'glodap/' in keyname and 'oxygen' in keyname: key.get_contents_to_filename(local_oxygen_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Don't run unless you want to grab a set of nine ARGO profile netcdf files (9 ARGO platforms (drifters (floats)))\n",
    "# f = ...strip strip strip strip is due to possible irregularities in the string cast of the key name\n",
    "# This may take a couple minutes to run\n",
    "\n",
    "import boto\n",
    "\n",
    "if False:\n",
    "    connection = boto.connect_s3(anon=True)\n",
    "    bucket = connection.get_bucket('himatdata')\n",
    "    for key in bucket.list(): \n",
    "        keyname = str(key.name.encode('utf-8'))\n",
    "        f = keyname.strip(\"b'\").strip('b\"').strip('\"').strip(\"'\")\n",
    "        if 'argo-profiles' in keyname: \n",
    "            ff = data_d + 'argo/' + f\n",
    "            key.get_contents_to_filename(ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Don't run unless you want to grab a large (800MB) tar file from S3 bucket 'oceanhackweek' to the local directory\n",
    "# This contains a bunch of different sub-dirs and data files as it un-tars into the 'data' directory.\n",
    "# This takes less than a minute to run.\n",
    "\n",
    "import boto\n",
    "if False:\n",
    "    f = '/home/jovyan/data.tar'\n",
    "    connection = boto.connect_s3(anon=True)\n",
    "    bucket = connection.get_bucket('oceanhackweek')\n",
    "    for key in bucket.list(): \n",
    "        keyname = str(key.name.encode('utf-8'))\n",
    "        if 'data.tar' in keyname: key.get_contents_to_filename(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto-copying /home/jovyan/data.tar.gz to an AWS bucket\n",
      ".........."
     ]
    },
    {
     "data": {
      "text/plain": [
       "1390518609"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's copy a file to an AWS S3 bucket: A tarfile backup of the data directory\n",
    "#   credentials and AWS S3 bucket name are read from a location outside of this repository\n",
    "#   credentials are used and then made inactive from the AWS console\n",
    "import boto\n",
    "import boto.s3\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "\n",
    "# Read connection credentials and bucket name from a file outside this repo\n",
    "authfile = open(home_d + '/creds/s3_creds','r')     \n",
    "line=authfile.readline().rstrip()                    # rstrip() removes any trailing \\n whitespace\n",
    "authfile.close()\n",
    "aws_id,aws_token,bucketname = line.split(',')\n",
    "conn = boto.connect_s3(aws_id, aws_token)\n",
    "bucket = conn.get_bucket(bucketname)\n",
    "transferfile = '/home/jovyan/data.tar.gz'\n",
    "print('boto-copying %s to an AWS bucket' % transferfile)\n",
    "\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = 'PC_data_dot_tar_dot_gz_03_JAN_2019'\n",
    "k.set_contents_from_filename(testfile, cb=percent_cb, num_cb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
