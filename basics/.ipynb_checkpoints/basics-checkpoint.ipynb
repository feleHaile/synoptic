{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basics\n",
    "\n",
    "\n",
    "This notebook is a bit of a mish-mash or melange of ideas and details that don't fit in elsewhere in the\n",
    "**synoptic** notebook ensemble. \n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "* Introduction\n",
    "* Data sources\n",
    "* Jupyter notebook shortcuts\n",
    "* Jupyter notebook interesting features\n",
    "* On installing Python packages\n",
    "* Using widgets\n",
    "* Graphs\n",
    "* Research notes\n",
    "* JupyterHub for curriculum \n",
    "\n",
    "\n",
    "The [synoptic notebooks](https://github.com/robfatland/synoptic) \n",
    "are centered around oceanography but have applicability to many forms of earth system data science.\n",
    "Port Cormorack is the associated Jupyter Hub where the various synoptic notebooks have been developed. Therefore if you\n",
    "are working through these notebooks and they are not working: There is most likely something in your set-up that is at\n",
    "variance with Port Cormorack.\n",
    "\n",
    "\n",
    "A related GitHub repository [pangeo-data/cmr](https://github.com/pangeo-data/cmr) includes notebooks on \n",
    "the NASA Commone Metadata Repository (CMR) and is therefore concerned with access to remote sensing datasets. \n",
    "\n",
    "\n",
    "### Data sources\n",
    "\n",
    "\n",
    "* [Heidi Sosik's Imaging Flow Cytometer (WHOI)](http://ifcb-data.whoi.edu/mvco)\n",
    "  * [Same but API-specific](http://ifcb-data.whoi.edu/api)\n",
    "* [BCO-DMO (WHOI)](https://www.bco-dmo.org/data)\n",
    "* ARGO\n",
    "  * [GitHub pyARGO](https://github.com/castelao/pyARGO) Rob is not convinced this is complete enough to be useful...\n",
    "  * [UCSD ARGO Informational FAQ](http://www.argo.ucsd.edu/Data_FAQ.html#RorD)\n",
    "  * [Coriolis map-based data selection tool](http://www.argodatamgt.org/Access-to-data/Argo-data-selection)\n",
    "  * [xarray lesson that works with ARGO data](https://rabernat.github.io/research_computing/xarray.html)\n",
    "* GLODAP\n",
    "* AQUA\n",
    "* TERRA\n",
    "* OOI CA CI\n",
    "* ECCO2\n",
    "* LIVE OCEAN\n",
    "* NANOOS\n",
    "* Crowd sourcing science efforts: \n",
    "[Braindr](https://braindr.us/#/), [Whaledr](https://whale-dr.firebaseapp.com/#/play), [appstract](https://appstract.pub/#/play)\n",
    "\n",
    "\n",
    "\n",
    "### Jupyter shortcuts\n",
    "\n",
    "\n",
    "[This blog post](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/): **really useful** Jupyter notebook shortcuts, e.g. *ESC* mode:\n",
    "\n",
    "\n",
    "- a + cell above\n",
    "- b + cell below\n",
    "- m cell is markdown\n",
    "- y cell is code\n",
    "- dd delete current cell\n",
    "- f Find and replace on code but not output\n",
    "- o Toggle cell output\n",
    "- shift + tab shows the docstring for an object you have just typed in a code cell\n",
    "  - Keep hitting tab to cycle through documentation\n",
    "- ctrl + shift splits the current cell in two at your cursor\n",
    "- shift + J or shift + down multi-select downwards\n",
    "- shift + K or shift + up multi-select upwards\n",
    "- shift + M to merge\n",
    "\n",
    "\n",
    "\n",
    "### Jupyter notebook interesting features\n",
    "\n",
    "\n",
    "- Jupyter notebooks can be managed as open source narratives (GitHub-backed) \n",
    "- Can treat local code as a simple package via 'import' on local-directory ```methods.py``` file\n",
    "- Cloud-backed JupyterHub can scale, manage authentication, access large datasets in object storage\n",
    "  - with / without containers\n",
    "  - ...or via **hosted environments**: SageMaker on AWS, AMLS + Azure Notebooks, CoLab on Google\n",
    "- Data deconstruction on NetCDF files: Learning how to get *at* the data\n",
    "- Widget-based interactivity: Sliders, buttons, movie-building and playback...\n",
    "- Additional rapidly emerging technologies (pursue at your peril!)\n",
    "  - Anaconda (is a Python environment and a package manager)\n",
    "  - Kubernetes (is container orchestration)\n",
    "  - HELM (automates Kubernetes configuration)\n",
    "  - Docker / DockerHub (containerized working environments)\n",
    "  - git / GitHub (version control, open source code sharing)\n",
    "  - binder \n",
    "  - TerraForm\n",
    "  - ...and so on...\n",
    "\n",
    "\n",
    "### On installing Python packages\n",
    "\n",
    "\n",
    "To run this notebook: pandas, netcdf4, xarray and boto must be installed.\n",
    "One way to get there is to invoke the command line as in:\n",
    "\n",
    "```\n",
    "!conda install netcdf4 -y\n",
    "!conda install xarray -y\n",
    "```\n",
    "\n",
    "However this is very stop-gap. Better is to enable the user to go to a terminal with sudo privileges. \n",
    "In this case we want to invoke a virtual Python environment called 'dev' where we create an associated\n",
    "installation of Python packages we want. This is the first step towards customizing a compute environment.\n",
    "\n",
    "\n",
    "```\n",
    "$ export PATH=/opt/anaconda/bin:$PATH\n",
    "$ source activate dev\n",
    "(dev) $ conda install networkx\n",
    "```\n",
    "\n",
    "- Done as a 'normal'\n",
    "- Installation is available to anyone using the **dev** kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is about enabling widgets; which is automatically working in Port Cormorack\n",
    "# Uncomment the next line and run this cell if necessary\n",
    "# !jupyter nbextension enable --py widgetsnbextension --sys-prefix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys          # used to halt the program\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random as r\n",
    "from numpy import zeros\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy.linalg\n",
    "\n",
    "# These imports give us control sliders that we use for selecting depth slices from the dataset\n",
    "from ipywidgets import *\n",
    "from traitlets import dlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs\n",
    "\n",
    "Graphs are definable as two sets: A first set of elements and a second set that associates pairs \n",
    "of elements from the first set. \n",
    "\n",
    "\n",
    "In what follows here we take advantage of the Python graph package **NetworkX** which has been \n",
    "pre-installed on the PortCormorack JupyterHub. The idea is to establish some use patterns on\n",
    "NetworkX in case these are of interest in the thematic geopspatial analysis (particularly oceanography)\n",
    "of *synoptic* and *cmr* repositories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1062e894304c4cd894be9eaa040210fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='type_choice', options={'Gnm random': 0, 'Erdos-Renyi random': 1, '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.DrawSomeGraph>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys          # used to halt the program\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random as r\n",
    "from numpy import zeros\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy.linalg\n",
    "\n",
    "# These imports give us control sliders that we use for selecting depth slices from the dataset\n",
    "from ipywidgets import *\n",
    "from traitlets import dlink\n",
    "\n",
    "# This creates a 2D color-coded view of oxygen at the surface, attaching a slider to a depth parameter\n",
    "def DrawSomeGraph(type_choice, n, control, layout):\n",
    "    \n",
    "    plt.figure(num=None, figsize=(6,6), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    \n",
    "    title_string = 'i was never given a title for this... odd'\n",
    "\n",
    "    probability = float(control)/100.0\n",
    "\n",
    "    if type_choice == 0: # G(n,m) chooses one possible graph with n vertices and m edges at random\n",
    "        m = control # number of edges\n",
    "        G = nx.gnm_random_graph(n, m)\n",
    "        title_string = 'Randomly chosen from among all graphs with m vertices and n edges'\n",
    "    elif type_choice == 1: \n",
    "        G = nx.random_graphs.erdos_renyi_graph(n, probability)\n",
    "        title_string = 'Erdos-Renyi random graph with probability ' + str(probability)\n",
    "    elif type_choice == 2:\n",
    "        # Use this graph to win a trophy at an information theory conference\n",
    "        G=nx.karate_club_graph()\n",
    "        title_string = 'The Karate club graph; go claim your trophy!'\n",
    "    elif type_choice == 3: \n",
    "        G = nx.heawood_graph()\n",
    "        title_string = 'Heawood graph, a cubic graph on 14 vertices with girth 6'\n",
    "    elif type_choice == 4:\n",
    "        k = min(n, 5)\n",
    "        G = nx.newman_watts_strogatz_graph(n, k, probability)\n",
    "        title_string = 'Small world graph'\n",
    "    elif type_choice == 5: \n",
    "        k = min(n, 10)\n",
    "        G = nx.powerlaw_cluster_graph(n, k, probability)\n",
    "        title_string = 'Power law cluster graph'\n",
    "    elif type_choice == 6: \n",
    "        k = int(control * n / 100)\n",
    "        if k < 1: k = 1\n",
    "        G = nx.barabasi_albert_graph(n, k)\n",
    "        title_string = 'Preferential attachment graph'\n",
    "    elif type_choice == 7: \n",
    "        G = nx.complete_graph(n)\n",
    "        title_string = 'complete graph'\n",
    "    elif type_choice == 8: \n",
    "        G = nx.complete_multipartite_graph(n, control)\n",
    "        title_string = 'complete bipartite n, m'\n",
    "    elif type_choice == 9: \n",
    "        # since the size parameter is a dimension (exponent) we auto-curtail this\n",
    "        if n > 7: n = 7\n",
    "        G = nx.hypercube_graph(n)\n",
    "        title_string = 'Hypercube graph where n is now the dimension'\n",
    "    elif type_choice == 10: \n",
    "        G = nx.random_lobster(n, probability, probability)\n",
    "        title_string = 'random lobster with both probabilities ' + str(probability)\n",
    "    else:\n",
    "        distance_threshold = float(control)/100.\n",
    "        G = nx.random_geometric_graph(n, distance_threshold)\n",
    "        title_string = 'random geometric using a threshold of ' + str(distance_threshold)\n",
    "    \n",
    "    # Use the layout parameter to choose an arrangement scheme\n",
    "    if layout == 0:\n",
    "        pos=nx.circular_layout(G)\n",
    "        title_string += '    (circular)'\n",
    "    elif layout == 1:\n",
    "        pos=nx.random_layout(G) \n",
    "        title_string += '    (random)'\n",
    "    elif layout == 2:\n",
    "        if type_choice == 9:\n",
    "            # Multiple shells hang on a hypercube \n",
    "            pos=nx.shell_layout(G)\n",
    "        else: \n",
    "            shells = []\n",
    "            nShells = 1\n",
    "            if n > 5: nShells += 1\n",
    "            if n > 11: nShells += 1\n",
    "            if n > 36: nShells += 1\n",
    "            if n > 72: nShells += 1\n",
    "            deltaShell = int(n / nShells)\n",
    "            # n = 43 gives nShells = 4, deltaShell = 10; 0-10, 10-20, 20-30, 30-40\n",
    "            for i in range(nShells):\n",
    "                shells.append(range(int(i*deltaShell), int((i+1)*deltaShell)))\n",
    "            if n > nShells * deltaShell: \n",
    "                shells.append(range(int(nShells*deltaShell), n))\n",
    "            pos=nx.shell_layout(G, shells) \n",
    "        title_string += '    (shell)'\n",
    "    elif layout == 3:\n",
    "        pos=nx.spring_layout(G) \n",
    "        title_string += '    (spring)'\n",
    "    elif layout == 4:\n",
    "        pos=nx.spectral_layout(G)\n",
    "        title_string += '    (spectral)'\n",
    "    else: # there is a 'graphviz' layout but not using this 'pos =' mechanism\n",
    "        pos=nx.circular_layout(G)\n",
    "        title_string += '    (circular)'\n",
    "    \n",
    "    # This does not work properly\n",
    "    # nx.draw(G)\n",
    "    cut = 1.1\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    xmin = cut * min(xx for xx, yy in pos.values())\n",
    "    ymin = cut * min(yy for xx, yy in pos.values())\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.title(title_string)\n",
    "\n",
    "    nx.draw_networkx(G,pos,with_labels=False,node_size=50)\n",
    "    # ,node_color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    # print(title_string)\n",
    "    \n",
    "    L = nx.normalized_laplacian_matrix(G)\n",
    "    e, v = numpy.linalg.eig(L.A)\n",
    "    e.sort()\n",
    "    for i in range(len(e)):\n",
    "        if e[i] < 1.0e-14: e[i] = 0\n",
    "    eivstring = \"Eigenvalues:\"\n",
    "    nTerms = min(n, 9)\n",
    "    for i in range(nTerms):\n",
    "        eivstring += '%.3f' % e[i] + ', '\n",
    "    if n > 10: eivstring += ', ..., '\n",
    "    eivstring += '%.3f' % e[n-1]\n",
    "    print(eivstring)\n",
    "    # plt.hist(e, bins=100)  # histogram with 100 bins\n",
    "    # plt.xlim(0, 2)  # eigenvalues between 0 and 2 \n",
    "\n",
    "# This is the interactive slider\n",
    "interact(DrawSomeGraph, \n",
    "            type_choice={'Gnm random': 0, \\\n",
    "                         'Erdos-Renyi random': 1, \\\n",
    "                         'Karate Club': 2, \\\n",
    "                         'Heawood Graph': 3, \\\n",
    "                         'Small world': 4, \\\n",
    "                         'Power law cluster': 5, \\\n",
    "                         'Preferential attachment': 6, \\\n",
    "                         'Complete': 7, \\\n",
    "                         'Complete Bipartite': 8, \\\n",
    "                         'Hypercube': 9, \\\n",
    "                         'Random Lobster': 10 \\\n",
    "                        }, \n",
    "            n=widgets.IntSlider(min=2,max=100,step=1,value=10, continuous_update=False),\n",
    "            control=widgets.IntSlider(min=0,max=100,step=1,value=50,continuous_update=False),\n",
    "            layout = {'circular': 0, \\\n",
    "                      'random': 1, \\\n",
    "                      'shell': 2, \\\n",
    "                      'spring': 3, \\\n",
    "                      'spectral': 4\n",
    "                        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research notes\n",
    "\n",
    "\n",
    "#### xarray features\n",
    "\n",
    "\n",
    "* apply operations over dimensions by name: ```x.sum('time')```\n",
    "* select values by label instead of integer location: x.loc['2014-01-01'] or x.sel(time='2014-01-01')\n",
    "* athematical operations (e.g., x - y) vectorize across multiple dimensions (array broadcasting) based on dimension names, not shape.\n",
    "* supports flexible split-apply-combine operations with groupby: x.groupby('time.dayofyear').mean().\n",
    "* database-like alignment based on coordinate labels that smoothly handles missing values\n",
    "  * x, y = xr.align(x, y, join='outer').\n",
    "* keep track of arbitrary metadata in the form of a Python dictionary: x.attrs.\n",
    "\n",
    "\n",
    "Note that pandas provides many of these features but does not make use of dimension names, and its core data structures are fixed dimensional arrays\n",
    "\n",
    "\n",
    "#### eScience incubator case study\n",
    "\n",
    "\n",
    "by Purshottam Shivraj\n",
    "\n",
    "\n",
    "* This project was an attempt to streamline workflow for optimizing physical parameters of Hydrological models (cf DHSVM)\n",
    "* Optimization was based on techniques like FAST, DECISION TREES and DREAM\n",
    "\n",
    "\n",
    "##### **Before** \n",
    "\n",
    "\n",
    "- Scientists manually tweak parameters for their model\n",
    "- For a large number of parameters the computation would fail\n",
    "\n",
    "\n",
    "##### **Plan**\n",
    "\n",
    "\n",
    "Distribute the computation workload across many machines to accelerate the optimization task; three stages:\n",
    "\n",
    "\n",
    "- FAST is a Fourier Amplitude Sensitivity Test, a variance-based sensitivity analysis method\n",
    "  - Idiosyncracy: Sample size required to achieve reasonable results is rather large\n",
    "  - Determines the sensitivity of the physical parameters\n",
    "  - Eventually narrows down the parameter space to only very sensitive parameters...\n",
    "    - ...i.e. those contributing the most towards predictive power of the model (metric: Nash–Sutcliffe) \n",
    "  - The bottleneck: Computationally expensive, requires multiple iterations\n",
    "- FAST results > decision tree (scikit-learn) to narrow the parameters further\n",
    "- Decision tree results > DREAM, an MCMC optimization method > convergence result\n",
    "\n",
    "\n",
    "##### **Result** \n",
    "\n",
    "- 11,000 model runs of DHSVM \n",
    "- 10 physical parameters: snow_threshold, lateral conductivity of water across landscape, etc\n",
    "- The most important parameters: *temperature lapse rate* and *lateral conductivity* of water across landscape\n",
    "- Local machine without parallelization: 1 month\n",
    "- Cloud instance (AWS) in parallel: 1 day \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### JupyterHub for curriculum\n",
    "\n",
    "- The Port Cormorack Jupyter Hub is oriented towards data-driven research\n",
    "- It is also -- however -- a potential curriculum resource; with these guidelines\n",
    "    - Teaching a class of **N** students will create a potential scale challenge for **N > 2** \n",
    "    - Each student should have a simple sign-on at a unique URL\n",
    "    - The students' work environment should be pre-populated with content... or with instructions for doing so\n",
    "    - Students start and modify notebooks on a persisting memory space (only; not one anothers' environments)\n",
    "    - It will be necessary to constrain and monitor spend (without involving students / credit cards)\n",
    "    - Some sort of recovery model should be in place should the persistent memory fail\n",
    "      - The most reliable is to have the students back up to GitHub repositories: An independent system\n",
    "    - Student access should not have more than a one-minute delay to connect\n",
    "    - There must be a mechanism for updating course material\n",
    "    - public cloud { **AWS**, **Google Cloud**, **Azure**, etcetera }\n",
    "    - Under the hood...\n",
    "       - Scaling technologies { docker, DockerHub, kubernetes, KOPS, HELM, ...}\n",
    "       - Replication technologies { JupyterHub, GitHub, nbgitpuller, ...} \n",
    "       - Storage {Each user has a persistent disk space or EFS or object storage buckets }\n",
    "       - Programming language including package manager { Python 3, Anaconda }\n",
    "       - Authentication: OAuth through Github, Google etc \n",
    "\n",
    "\n",
    "#### Two contrasting JupyterHub deployment methods\n",
    "\n",
    "\n",
    "1. A single giant server where packages and environments are installed directly onto the machine via pip install or conda install. Pros: easy to set up, can quickly install packages as the need arises. Cons: works for small groups, \n",
    "constrained memory/compute capacity. \n",
    "\n",
    "  * Provision a large server - say one with 48 cpus and 192 GB memory. Follow the steps outlined in https://jupyterhub.readthedocs.io/en/latest/installation-guide.html to install jupyterhub. Thereafter, packages \n",
    "and libraries can be installed on the server using the traditional ```pip install``` or ```conda install```. \n",
    "This will propagate as **kernels** on Jupyterhub. Each user is assigned a home directory on the server which \n",
    "corresponds to their login username (i.e. if Github is the authentication mechanism then the home directories \n",
    "should correspond exactly to the Github username).\n",
    "\n",
    "2. Scalable using a container management and load balancing.  Container management = Docker + Kubernetes + Helm + KOPS... \n",
    "Pros: scalable, can accommodate a large number of participants. Cons: uses docker images to create environments, therefore \n",
    "needs a longer lead time to ensure all packages, libraries and dependencies are installed; not very improvisational\n",
    "\n",
    "\n",
    "  * See [this link](https://zero-to-jupyterhub-with-kubernetes.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "  * Instead of the giant server (generating waste heat much of the time) the number of servers can be scaled up or \n",
    "down as required. \n",
    "\n",
    "  * Four technologies:\n",
    "\n",
    "    * Kubernetes which is a manager for the deployment and scaling of containers. Kubernetes clusters are the foundation of the scalable Jupyterhub ecosystem -- it consists of a master node where the Jupyterhub spawner resides and worker nodes where “pods” or docker containers are spawned. \n",
    "\n",
    "    * Kubernetes clusters are provisioned on Google Cloud Platform using its native API, Azure using Azure container services and AWS using two methods - KOPS (Kubernetes operations service) or a quickstart Heptio Template. KOPS autoscales while deployment of kubernetes clusters using the Heptio template does NOT autoscale. \n",
    "\n",
    "\n",
    "    * Once the kubernetes cluster is provisioned, the kubernetes command tool (kubectl) needs to be installed \n",
    "in order to interact the kubernetes cluster. The distinction between kubectl and kubernetes management tools \n",
    "like KOPs is that kubectl cannot provision a kubernetes cluster. It is mainly used to issue commands TO a pre-made \n",
    "kubernetes cluster.  Kubectl helps you get cluster information like node IPs and # of pods or to figure out if \n",
    "something went wrong with your kubernetes cluster. Jupyterhub is installed on the kubernetes cluster using \n",
    "helm. Helm is an application manager for kubernetes. Once a user logs on to your scalable Jupyterhub, a “pod” \n",
    "starts up. What a pod is, is essentially a docker container. Jupyterhub pulls a docker image from an image that \n",
    "you have created; this image contains all your packages and libraries that you want to use for your class. \n",
    "The path to this image is specified in a configuration file while installing Jupyterhub via helm. This docker \n",
    "image can also contain *some* data though not encouraged to be large datasets. \n",
    "\n",
    "    * ```helm install jupyterhub``` to install jupyterhub on the kubernetes cluster\n",
    "\n",
    "\n",
    "#### Docker\n",
    "\n",
    "\n",
    "Also requisite. Stipulate how much CPU, memory and disk space to allocate to each user. \n",
    "This will factor into the size and number of machines in the cluster. For example let’s say you have \n",
    "150 students in your course and each student will be using 4 cpus and 6GB of memory. This will mean \n",
    "that your maximum cluster size should be about 17 c4.8xlarge instances on AWS. That will allocate \n",
    "space for about 9 pods per instance. \n",
    "\n",
    "\n",
    "\n",
    "#### Democratize the process\n",
    "\n",
    "\n",
    "\n",
    "* Create a **Deployjhub** package to auto-provision the kubernetes cluster, install jupyterhub, deploy the docker image\n",
    "* Train teaching assistants to spin up instances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
